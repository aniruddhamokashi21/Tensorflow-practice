{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Implement a backpropagation algorithm to train a DNN with at least 2 hidden layers.**"
      ],
      "metadata": {
        "id": "9QQiSXgL9Ap4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from keras.activations import sigmoid\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "V0GG64b69Li9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropogation"
      ],
      "metadata": {
        "id": "Rlt9EqWHEscl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron(object):\n",
        "    '''\n",
        "    params: links: array of weights connected to the neuron from previous layer\n",
        "            is_input: (boolean) tells whether 'this' neuron is an input neuron or not.\n",
        "            activation_type: Type of activation function\n",
        "    '''\n",
        "    def __init__(self, is_input=False, activation_type='sigmoid'):\n",
        "        self.activation = 0.0\n",
        "        self.is_input = is_input\n",
        "        self.error_term = 0.0 # Delta for each neuron\n",
        "\n",
        "        if activation_type == 'sigmoid':\n",
        "            self.activation_fn = sigmoid\n",
        "\n",
        "    def __str__(self):\n",
        "        output_str = \"<< Activation value: {}, Error term: {} >>\".format(self.activation, self.error_term)\n",
        "        return output_str\n",
        "\n",
        "    def get_activation(self):\n",
        "        return self.activation\n",
        "\n",
        "    def set_error_term(self, value):\n",
        "        self.error_term = value\n",
        "\n",
        "    def get_error_term(self):\n",
        "        return self.error_term\n",
        "\n",
        "    # prev_inputs: 'x', weights: 'w'. Take dot product of these two.\n",
        "    def activate_neuron(self, prev_inputs, weights, bias_value):\n",
        "        # sum_value = np.dot(prev_inputs, self.links)\n",
        "        if weights is None:\n",
        "            # input layer. 'prev_inputs' is now the actual activation value (from the input_vector)\n",
        "            self.activation = prev_inputs\n",
        "        else:\n",
        "            sum_value = np.dot(prev_inputs, weights) + bias_value\n",
        "            activation_val = self.activation_fn(sum_value)\n",
        "            self.activation = activation_val\n",
        "\n",
        "\n",
        "class Layer(object):\n",
        "\n",
        "    def __init__(self, layer_properties, prev_layer_neurons):\n",
        "        self.num_units = layer_properties['neuron_count']\n",
        "        self.layer_type = layer_properties['layer_type']\n",
        "        self.layer_ID = layer_properties['layer_ID']\n",
        "        self.activation_type = layer_properties['activation_type']\n",
        "        self.layer_number = layer_properties['layer_number']\n",
        "        self.prev_layer_neurons = prev_layer_neurons\n",
        "\n",
        "\n",
        "        # Initialize Neurons in the layer and,\n",
        "        # Initialize the weight matrix with random weights\n",
        "        # The weight matrix is weights between 'this' and previous layer\n",
        "\n",
        "        # For hidden and output layers\n",
        "        if self.layer_type == 'h' or self.layer_type == 'o':\n",
        "            self.weight_matrix = np.random.uniform(low=-0.5, high=0.5, size=(self.num_units, prev_layer_neurons))\n",
        "            self.biases = np.random.uniform(low=-0.5, high=0.5, size=(self.num_units,))\n",
        "            self.neurons = [Neuron() for i in range(self.num_units)]\n",
        "        # For input layer\n",
        "        elif self.layer_type == 'i':\n",
        "            self.weight_matrix = None\n",
        "            self.biases = None\n",
        "            self.neurons = [Neuron(is_input=True, activation_type='sigmoid') for i in range(self.num_units)]\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weight_matrix\n",
        "\n",
        "    def get_layer_ID(self):\n",
        "        return self.layer_ID\n",
        "\n",
        "    def get_layer_type(self):\n",
        "        return self.layer_type\n",
        "\n",
        "    def get_weights_for_neuron(self, index):\n",
        "        # return weight vector for neuron at index 'index'\n",
        "        return self.weight_matrix[index]\n",
        "\n",
        "    def get_bias_value(self, index):\n",
        "        return self.biases[index]\n",
        "\n",
        "    def get_neuron_activations(self):\n",
        "        return np.array([n.get_activation() for n in self.neurons])\n",
        "        # returns Array of activations. shape = (n,)\n",
        "\n",
        "    def get_neuron_count(self):\n",
        "        return len(self.neurons)\n",
        "\n",
        "    def get_neuron_error_terms(self):\n",
        "        return np.array([n.get_error_term() for n in self.neurons])\n",
        "\n",
        "    # This following function 'calculate_error_terms()' works in 2 types:\n",
        "    # 1) if 'is_output' == True, That means error term calculation will be according to output layer formula. 'resource' is target_output_vector here\n",
        "    # 2) if 'is_output  == False, That means error term calculation is for hidden layer. 'resource' here is tuple of (weights, error_vec) between 'this' layer and 'next' layer.\n",
        "    # read the README file for better understanding.\n",
        "    def calculate_error_terms(self, is_output, resource):\n",
        "        if is_output == True:\n",
        "            # 'resource' is target output vector\n",
        "            for n in range(len(self.neurons)):\n",
        "                # delta = o * (1 - o) * (t - o)\n",
        "                error_value = self.neurons[n].get_activation() * (1 - self.neurons[n].get_activation()) * (resource[n] - self.neurons[n].get_activation())\n",
        "                self.neurons[n].set_error_term(error_value)\n",
        "        else:\n",
        "            # 'resource' is now weight matrix!\n",
        "            (weight_matrix, error_vector) = resource\n",
        "            for n in range(len(self.neurons)):\n",
        "                temp_sum = np.dot(weight_matrix.T[n], error_vector)\n",
        "                error_value = self.neurons[n].get_activation() * (1 - self.neurons[n].get_activation()) * temp_sum\n",
        "                self.neurons[n].set_error_term(error_value)\n",
        "\n",
        "    # X is previous layer input, DELTA is vector of error terms\n",
        "    def update_weights(self, X, DELTA, learning_rate):\n",
        "        # Call this function only in backward pass.\n",
        "        if self.layer_type != 'i':\n",
        "            del_W = np.zeros(shape=self.weight_matrix.shape)\n",
        "            for i in range(len(X)):\n",
        "                for j in range(len(del_W)):\n",
        "                    del_W[j,i] = learning_rate * X[i] * DELTA[j]\n",
        "            self.weight_matrix = self.weight_matrix + del_W\n",
        "            '''\n",
        "            Eliminate these Python double loops!!\n",
        "\n",
        "            '''\n",
        "\n",
        "\n",
        "    def print_layer_properties(self):\n",
        "        print('**************')\n",
        "        print('Layer Number: {}\\nLayer ID: {}\\nLayer Type: {}\\nNeuron count: {}\\nActivation type: {}'.format(\n",
        "            self.layer_number,\n",
        "            self.layer_ID,\n",
        "            self.layer_type,\n",
        "            self.num_units,\n",
        "            self.activation_type\n",
        "        ))\n",
        "        print('**************')\n",
        "\n",
        "    def print_layer_neurons(self):\n",
        "        print('Layer {}'.format(self.layer_number))\n",
        "        temp = []\n",
        "        for i in range(self.num_units):\n",
        "            temp.append(self.neurons[i].get_activation())\n",
        "        print(temp)\n",
        "        print('==============================')\n",
        "        #     print('{})'.format(i+1))\n",
        "        #     print(self.neurons[i])\n",
        "        #     print('****************')\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "\n",
        "    def __init__(self, network_file_path):\n",
        "        # Get network settings, from network config file\n",
        "        with open(network_file_path, \"r\") as net:\n",
        "            self.network_properties = json.load(net)\n",
        "\n",
        "        print(\"Loaded network file: {}\".format(self.network_properties[\"network_name\"]))\n",
        "        self.layers = [] # Array of layers\n",
        "        prev_layer_units = None\n",
        "        for i in range(self.network_properties['n_layers']+1):\n",
        "            current_layer = self.network_properties['layers'][i]\n",
        "            self.layers.append(Layer(current_layer, prev_layer_units))\n",
        "            prev_layer_units = current_layer['neuron_count']\n",
        "        # Load dataset here.\n",
        "        df = pd.read_csv(self.network_properties['dataset_path'])\n",
        "        data = np.array(df)\n",
        "        data_block = data[:, 1:]\n",
        "        self.train, self.test = train_test_split(data_block, train_size=0.8, random_state=42)\n",
        "        trainX = self.train[:, :self.network_properties['input_size']]\n",
        "        trainY = self.train[:, self.network_properties['input_size']:]\n",
        "        size = len(self.train)\n",
        "        self.training_data = (trainX, trainY, size)\n",
        "        print(\"Initially layers are:\")\n",
        "        self.print_layers()\n",
        "\n",
        "    def print_training_data(self):\n",
        "        print(\"data: \")\n",
        "        print(self.training_data)\n",
        "\n",
        "    def print_layers(self):\n",
        "        print('printing all the layers:')\n",
        "        for i in range(self.network_properties['n_layers']+1):\n",
        "            self.layers[i].print_layer_neurons()\n",
        "            print('#################')\n",
        "\n",
        "    def forward_pass(self, input_vector, return_value=False):\n",
        "\n",
        "        # print(\"Forward pass: input='{}'\".format(input_vector))\n",
        "        # Input vector should be of same length as the number of neurons in input layer.\n",
        "        assert len(input_vector)==self.layers[0].get_neuron_count()\n",
        "\n",
        "        # For every layer after input layer,\n",
        "        previous_layer_input = input_vector\n",
        "        for layer in self.layers:\n",
        "            if layer.get_layer_type() == 'i':\n",
        "                for n in np.arange(layer.get_neuron_count()):\n",
        "                    layer.neurons[n].activate_neuron(input_vector[n], None, None)\n",
        "            else:\n",
        "                temp = []\n",
        "                # Calculate the weighted sum for every neuron\n",
        "                for n in np.arange(layer.get_neuron_count()):\n",
        "                    # pass the required vectors (x, w) to the activate_neuron() method\n",
        "                    layer.neurons[n].activate_neuron(previous_layer_input, layer.get_weights_for_neuron(n), layer.get_bias_value(n))\n",
        "                    # accumulate the current activation values\n",
        "                    temp.append(layer.neurons[n].get_activation())\n",
        "\n",
        "                # Update the previous layer input, which will be fed to the next layer\n",
        "                previous_layer_input = np.array(temp)\n",
        "                temp.clear()\n",
        "        if return_value == True:\n",
        "            return previous_layer_input\n",
        "\n",
        "    def backward_pass(self, target_output_vector):\n",
        "        error_vector = None\n",
        "        weight_matrix = None\n",
        "        # Calculate Error terms for all layers (end to start):\n",
        "        for current_layer in self.layers[::-1]:\n",
        "            if current_layer.get_layer_type() == 'o':\n",
        "                current_layer.calculate_error_terms(True, target_output_vector)\n",
        "            elif current_layer.get_layer_type() == 'h':\n",
        "                current_layer.calculate_error_terms(False, (weight_matrix, error_vector))\n",
        "            else:\n",
        "                # If current layer is 'input layer' then stop processing\n",
        "                break\n",
        "            error_vector = current_layer.get_neuron_error_terms()\n",
        "            weight_matrix = current_layer.get_weights()\n",
        "\n",
        "        # Update every weight now:\n",
        "        previous_layer_neuron_activations = None\n",
        "        for layer in self.layers:\n",
        "            if layer.get_layer_type() != 'i':\n",
        "                layer.update_weights(previous_layer_neuron_activations, layer.get_neuron_error_terms(), float(self.network_properties['learning_rate']))\n",
        "            previous_layer_neuron_activations = layer.get_neuron_activations()\n",
        "\n",
        "    def calculate_error(self):\n",
        "        pass\n",
        "\n",
        "    # Train the network using back propagation algorithm\n",
        "    def train_network(self):\n",
        "        X_train, y_train, size = self.training_data\n",
        "\n",
        "        for epoch in range(self.network_properties[\"epochs\"]):\n",
        "            print(\"Epoch {}\".format(epoch+1))\n",
        "            for i in tqdm(range(size)):\n",
        "                # print('Training example {}'.format(i+1))\n",
        "                # print('train_X: {}\\ntrain_y: {}'.format(X_train[i], y_train[i]))\n",
        "                self.forward_pass(X_train[i])\n",
        "                self.backward_pass(y_train[i])\n",
        "                # print(\"iteration completed. Result:\")\n",
        "                # self.print_layers()\n",
        "\n",
        "\n",
        "    def predict_answer(self):\n",
        "        testX, testY = self.test[:, :self.network_properties['input_size']], self.test[:, self.network_properties['input_size']:]\n",
        "        for i in range(len(self.test)):\n",
        "            prediction_vector = self.forward_pass(testX[i], return_value=True)\n",
        "            # print(prediction_vector)\n",
        "            print(\"Prediction: {} Actual: {}\".format(np.argmax(prediction_vector), np.argmax(testY[i])))"
      ],
      "metadata": {
        "id": "n1HwSQsYEs2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation function"
      ],
      "metadata": {
        "id": "vPXuJ_uhEtMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return 0 if x <= 0 else x\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def leaky_relu(x):\n",
        "    return 0.1*x if x <= 0 else x\n",
        "\n",
        "def threshold(x, t=0):\n",
        "    return 0 if x <= t else 1"
      ],
      "metadata": {
        "id": "N0jmQeLGEtoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset creator"
      ],
      "metadata": {
        "id": "oKA5QxFkEuQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "\n",
        "def dataset_creator():\n",
        "    rows = int(input(\"Enter no. of training examples you want: \"))\n",
        "    train_size = int(input(\"Enter size of input vector: \"))\n",
        "    test_size = int(input(\"Enter size of target output vector: \"))\n",
        "    cols = train_size + test_size\n",
        "    data_block = np.random.choice([0, 1], size=(rows, cols), p=[0.5, 0.5])\n",
        "    print(\"Your data block has been created, first {} columns are training examples, last {} columns are target outputs\".format(train_size, test_size))\n",
        "    done = False\n",
        "    file_name = None\n",
        "    while done != True:\n",
        "        file_name = input(\"Enter name of the file you want to save it as: \")\n",
        "        if len(file_name) != 0:\n",
        "            done = True\n",
        "        else:\n",
        "            print(\"Try again, enter valid path!\")\n",
        "\n",
        "    df = pd.DataFrame(data_block)\n",
        "    df.to_csv(join('Input', file_name+\".csv\"))\n",
        "    print(\"File name {}.csv is saved in 'Input' directory\".format(file_name))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_creator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "SkcyfJK2Eupg",
        "outputId": "0c67007e-60ad-4502-8dbd-c3d2da5ba615"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter no. of training examples you want: 5\n",
            "Enter size of input vector: 5\n",
            "Enter size of target output vector: 5\n",
            "Your data block has been created, first 5 columns are training examples, last 5 columns are target outputs\n",
            "Enter name of the file you want to save it as: iris\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a412440645af>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdataset_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-a412440645af>\u001b[0m in \u001b[0;36mdataset_creator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File name {}.csv is saved in 'Input' directory\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3718\u001b[0m         )\n\u001b[1;32m   3719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3720\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         )\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'Input'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iris dataset making"
      ],
      "metadata": {
        "id": "SjBoUZN6HAQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DATA_DIR = os.path.join(os.getcwd(), 'Input')\n",
        "\n",
        "df = pd.read_csv(os.path.join(DATA_DIR, \"/content/iris_dataset.csv\"))\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "columns = df.columns\n",
        "\n",
        "unique_species = df['Species'].unique()\n",
        "\n",
        "categorical_vectors = {unique_species[0]: [1,0,0], unique_species[1]: [0,1,0], unique_species[2]: [0,0,1]}\n",
        "\n",
        "print(\"loading data...\")\n",
        "data = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
        "\n",
        "print(\"Making categorical (One hot) vectors...\")\n",
        "c = []\n",
        "for i in df['Species'].values:\n",
        "    c.append(categorical_vectors[i])\n",
        "\n",
        "c = np.array(c)\n",
        "print(\"Creating final data block...\")\n",
        "final_data = np.hstack([data, c])\n",
        "\n",
        "print(\"Saving final data block as 'iris_dataset_final.csv' in {}\".format(DATA_DIR))\n",
        "final_iris_data = pd.DataFrame(final_data)\n",
        "final_iris_data.to_csv(os.path.join(DATA_DIR, \"iris_dataset_final.csv\"))\n",
        "print(\"The dataset is saved as '{}'\".format(os.path.join(DATA_DIR, \"iris_dataset_final.csv\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "tAzll_HsHAm_",
        "outputId": "7a90a2d4-2f47-47f9-e1fb-2a2e039cc3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
            "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
            "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
            "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
            "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
            "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n",
            "loading data...\n",
            "Making categorical (One hot) vectors...\n",
            "Creating final data block...\n",
            "Saving final data block as 'iris_dataset_final.csv' in /content/Input\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b3a29227e9cc>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving final data block as 'iris_dataset_final.csv' in {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mfinal_iris_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mfinal_iris_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iris_dataset_final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The dataset is saved as '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iris_dataset_final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3718\u001b[0m         )\n\u001b[1;32m   3719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3720\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         )\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/content/Input'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runner"
      ],
      "metadata": {
        "id": "eSjvlks0EoS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from BackProp import NeuralNetwork, Layer, Neuron\n",
        "from dataset_creator import dataset_creator\n",
        "\n",
        "NETWORK_DIRECTORY = os.path.join(os.getcwd(), 'Network_structures')\n",
        "\n",
        "print(\"Welcome to BackProp simulation\")\n",
        "print(\"The network will be loaded from a JSON file, which you can provide\")\n",
        "print(\"Some sample testing JSON files are given, refer those to make your own custom network\")\n",
        "\n",
        "network = NeuralNetwork(os.path.join(NETWORK_DIRECTORY, 'network_2.json'))\n",
        "\n",
        "network.train_network()\n",
        "network.predict_answer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "3V3YiTAZEMow",
        "outputId": "3dd3d2c9-5b55-4362-daf6-965344e79eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-529d39259205>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mBackProp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNeuron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset_creator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'BackProp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}